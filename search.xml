<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes架构]]></title>
    <url>%2F2018%2F12%2F28%2Fkubernetes%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[k8s架构节选自 https://draveness.me/understanding-kubernetes 请大家到原网站观看更多内容 Kubernetes 遵循非常传统的客户端服务端架构，客户端通过 RESTful 接口或者直接使用 kubectl 与 Kubernetes 集群进行通信，这两者在实际上并没有太多的区别，后者也只是对 Kubernetes 提供的 RESTful API 进行封装并提供出来。 每一个 Kubernetes 就集群都由一组 Master 节点和一系列的 Worker 节点组成，其中 Master 节点主要负责存储集群的状态并为 Kubernetes 对象分配和调度资源。 Master作为管理集群状态的 Master 节点，它主要负责接收客户端的请求，安排容器的执行并且运行控制循环，将集群的状态向目标状态进行迁移，Master 节点内部由三个组件构成： 其中 API Server 负责处理来自用户的请求，其主要作用就是对外提供 RESTful 的接口，包括用于查看集群状态的读请求以及改变集群状态的写请求，也是唯一一个与 etcd 集群通信的组件。 而 Controller 管理器运行了一系列的控制器进程，这些进程会按照用户的期望状态在后台不断地调节整个集群中的对象，当服务的状态发生了改变，控制器就会发现这个改变并且开始向目标状态迁移。 最后的 Scheduler 调度器其实为 Kubernetes 中运行的 Pod 选择部署的 Worker 节点，它会根据用户的需要选择最能满足请求的节点来运行 Pod，它会在每次需要调度 Pod 时执行。 Worker其他的 Worker 节点实现就相对比较简单了，它主要由 kubelet 和 kube-proxy 两部分组成： kubelet 是一个节点上的主要服务，它周期性地从 API Server 接受新的或者修改的 Pod 规范并且保证节点上的 Pod 和其中容器的正常运行，还会保证节点会向目标状态迁移，该节点仍然会向 Master 节点发送宿主机的健康状况。 另一个运行在各个节点上的代理服务 kube-proxy 负责宿主机的子网管理，同时也能将服务暴露给外部，其原理就是在多个隔离的网络中把请求转发给正确的 Pod 或者容器。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes总结]]></title>
    <url>%2F2018%2F12%2F28%2Fkubernetes%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[kubernetes 总结基础0x01 kubernetes包含几个组件。各个组件的功能是什么。组件之间是如何交互的Masterkube-apiserver暴露整个集群的API，任何资源请求/调用操作都是通过调用API来实现，唯一一个跟ETCD通信的组件 ETCD存储整个集群的信息 kube-scheduler负责对于新建pod的调度，基于node节点的软件硬件负载和亲和性来选择节点 kube-controller-manager运行在master节点的控制器的集合 Node Controller 监控和相应节点down Replication Controller 为每个RC对象维护正确数量的pod Endpoints Controller 负责建立Endpoints，链接Service和Pods Service Account &amp; Token Controllers 为新的Namespace创建默认账户和API访问Token cloud-controller-manager负责和底层云服务交互(可选) Node Controller Route Controller Service Controller Volume Controller Nodekubelet运行与每个节点，根据下发的PodSpecs确保Pod中的容器的运行和健康 kube-proxy维护主机上kube服务涉及的网络规则和连接转发，负责service的实现,采用iptables(Nat/Filter) 侦听service更新事件，并更新service相关的iptables规则 侦听endpoint更新事件，更新endpoint相关的iptables规则（如 KUBE-SVC-链中的规则），然后将包请求转入endpoint对应的Pod Container Runtime容器运行时，支持Docker rkt runc AddonDNS集群的DNS，负责内部服务的DNS解析 dashboardContainer Resource Monitoring容器资源监控 Cluster-level Logging 0x02 k8s的pause容器有什么用。是否可以去掉。pause容器是pod级别的，每个pod中的pause容器为业务容器的父容器 共享命名空间 IPC Hostname Network PID 而是提供Pid Namespace并使用init进程。 0x03 k8s中的pod内几个容器之间的关系是什么都由Pod中的pause容器创建，共享network和volume资源 0x04 一个经典pod的完整生命周期。 Pending：Pod 定义正确，提交到 Master，但其包含的容器镜像还未完全创建。通常处在 Master 对 Pod 的调度过程中。 ContainerCreating：Pod 的调度完成，被分配到指定 Node 上。处于容器创建的过程中。通常是在拉取镜像的过程中。 Running：Pod 包含的所有容器都已经成功创建，并且成功运行起来。 Successed：Pod 中所有容器都成功结束，且不会被重启。这是 Pod 的一种最终状态。 Failed：Pod 中所有容器都结束，但至少一个容器以失败状态结束。这也是 Pod 的一种最终状态。 k8s的service和ep是如何关联和相互影响的Service 通过selector选择pod,生成endpoint，并通过endpoint和后端pod通信kube-proxy 监听到service被删除，则删除和该service同名的endpoint对象 监听到新的service被创建，则根据新建service信息获取相关pod列表，然后创建对应endpoint对象 监听到service被更新，则根据更新后的service信息获取相关pod列表，然后更新对应endpoint对象 监听到pod事件，则更新对应的service的endpoint对象，将podIp记录到endpoint中 详述kube-proxy原理kube-proxy iptables iptables - netfilter - NAT/FILTER 访问 nodePort - clusterIp:port - Pod:portKubernetes通过在目标node的iptables中的nat表的PREROUTING和POSTROUTING链中创建一系列的自定义链 （这些自定义链主要是“KUBE-SERVICES”链、“KUBE-POSTROUTING”链、每个服务所对应的“KUBE-SVC-XXXXXXXXXXXXXXXX”链和“KUBE-SEP-XXXXXXXXXXXXXXXX”链），然后通过这些自定义链对流经到该node的数据包做DNAT和SNAT操作以实现路由、负载均衡和地址转换。 iptables -S -t nat rc/rs功能是怎么实现的。详述从API接收到一个创建rc/rs的请求，到最终在节点上创建pod的全过程，尽可能详细。另外，当一个pod失效时，kubernetes是如何发现并重启另一个pod的？Deployment 而当前推荐的做法是使用Deployment+ReplicaSetRC是通过ReplicationManager监控RC和RC内Pod的状态，从而增删Pod，以实现维持特定副本数的功能。 deployment/rs有什么区别。其使用方式、使用条件和原理是什么。deployment控制rs，rs控制poddeployment是rs的超集，提供更多的部署功能，如：回滚、暂停和重启、 版本记录、事件和状态查看、滚动升级和替换升级。 group中的cpu有哪几种限制方式。k8s是如何使用实现request和limit的。cpu.cfs_quota_us、cpu.cfs_period_us 限制CPU的使用时间cpu.shares 限制group的配额 request 容器使用最小资源limit 容器使用的最大资源k8s中资源使用 拓展实践设想一个一千台物理机，上万规模的容器的kubernetes集群，请详述使用kubernetes时需要注意哪些问题？应该怎样解决？（提示可以从高可用，高性能等方向，覆盖到从镜像中心到kubernetes各个组件等）集群规模庞大时要做得限制 配额问题针对云厂商存储在独立的etcd集群中管理节点和组件的规格插件资源占用限制 一千到五千台的瓶颈京东大规模集群实现 1-5000 kubernetes的运行中有哪些注意的要点。集群发生雪崩的条件，以及预防手段。京东大规模k8s集群的精细化运营 设计一种可以替代kube-proxy的实现sidecar的设计模式如何在k8s中进行应用。有什么意义。sidecar模式将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。 灰度发布是什么。如何使用k8s现有的资源实现灰度发布。介绍使用istio实现k8s的灰度发布 介绍k8s实践中踩过的比较大的一个坑和解决方式。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
</search>
